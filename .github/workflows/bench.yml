name: Benchmark

on:
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: macos-15
    permissions:
      contents: read
      actions: write

    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Rust Tools
        run: cargo install critcmp

      - name: Run Rust Benchmarks
        run: |
          cd memorypack-benchmarks/rust-benchmarks
          cargo bench --bench memorypack_bench -- --save-baseline current
          critcmp --export current > ../../rust_results.json

      - name: Restore .NET
        run: |
          cd memorypack-benchmarks/dotnet-benchmarks
          dotnet restore

      - name: Run .NET Benchmarks
        run: |
          cd memorypack-benchmarks/dotnet-benchmarks
          dotnet run -c Release

      - name: Locate .NET Results
        id: dotnet-locate
        run: |
          echo "Searching for BenchmarkDotNet results..."
          find memorypack-benchmarks/dotnet-benchmarks/BenchmarkDotNet.Artifacts -type f -name "*.json" 2>/dev/null || true
          JSON_PATH=$(find memorypack-benchmarks/dotnet-benchmarks/BenchmarkDotNet.Artifacts/results -name "*-report-full.json" -type f 2>/dev/null | head -n 1)
          if [ -z "$JSON_PATH" ]; then
            JSON_PATH=$(find memorypack-benchmarks/dotnet-benchmarks/BenchmarkDotNet.Artifacts/results -name "*.json" -type f 2>/dev/null | head -n 1)
          fi
          if [ -z "$JSON_PATH" ]; then
            echo "No JSON results found"
            exit 1
          fi
          echo "Found: $JSON_PATH"
          cp "$JSON_PATH" dotnet_results.json

      - name: Generate Comparison Table
        shell: bash
        run: |
          python3 <<EOF >> $GITHUB_STEP_SUMMARY
          import json

          def fmt_ns(ns):
              if ns >= 1000: return f"{ns/1000:.2f} us"
              return f"{ns:.2f} ns"

          def normalize_name(name):
              return name.lower().replace('_', '')

          try:
              with open('rust_results.json') as f:
                  rust_data = json.load(f)

              with open('dotnet_results.json') as f:
                  dotnet_raw = json.load(f)

              rust_map = {}

              # critcmp export format: { "name": "current", "benchmarks": { "bench_name": { "criterion_estimates_v1": { "mean": { "point_estimate": ... } } } } }
              benchmarks = rust_data.get('benchmarks', {})
              for bench_name, bench_data in benchmarks.items():
                  estimates = bench_data.get('criterion_estimates_v1', {})
                  mean_data = estimates.get('mean', {})
                  point_estimate = mean_data.get('point_estimate')
                  if point_estimate is not None:
                      normalized = normalize_name(bench_name)
                      rust_map[normalized] = point_estimate

              comparison = []
              for d in dotnet_raw['Benchmarks']:
                  csharp_name = d['Method']
                  match_key = normalize_name(csharp_name)
                  csharp_mean = d['Statistics']['Mean']
                  rust_mean = rust_map.get(match_key)
                  comparison.append({"Test": csharp_name, "C#": csharp_mean, "Rust": rust_mean})

              print("")
              print("## Benchmark Summary")
              print("")
              print("| Test Case | C# Time | Rust Time | Winner | Ratio |")
              print("| :--- | :---: | :---: | :---: | :---: |")
              for row in comparison:
                  c_time = row['C#']
                  r_time = row['Rust']
                  if r_time is None:
                      print(f"| {row['Test']} | {fmt_ns(c_time)} | N/A | - | - |")
                      continue
                  if r_time == 0 or c_time == 0:
                      print(f"| {row['Test']} | {fmt_ns(c_time)} | {fmt_ns(r_time)} | - | - |")
                      continue
                  diff = c_time / r_time
                  winner = "Rust" if r_time < c_time else "C#"
                  ratio = f"{diff:.2f}x" if r_time < c_time else f"{r_time/c_time:.2f}x"
                  print(f"| {row['Test']} | {fmt_ns(c_time)} | {fmt_ns(r_time)} | {winner} | {ratio} |")
          except Exception as e:
              import traceback
              print(f"Error parsing benchmarks: {e}")
              print("```")
              traceback.print_exc()
              print("```")
          EOF

      - name: Download previous baseline
        uses: dawidd6/action-download-artifact@v11
        id: download-baseline
        continue-on-error: true
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          workflow: bench.yml
          workflow_conclusion: success
          name: benchmark-baseline
          path: old_baseline
          search_artifacts: true
          if_no_artifact_found: warn

      - name: Check for Regressions
        if: steps.download-baseline.outcome == 'success'
        run: |
          if [ -f old_baseline/rust_results.json ]; then
            echo "Previous baseline found. Comparison available in future updates." >> $GITHUB_STEP_SUMMARY
          else
            echo "No previous baseline file found." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload New Baseline
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline
          path: rust_results.json
          overwrite: true
